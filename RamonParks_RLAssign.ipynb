{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4yvI-kf-myN"
      },
      "source": [
        "# MDPs and Q-learning On \"Ice\" (60 points possible)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iEuDHFs-ixs"
      },
      "source": [
        "In this assignment, we’ll revisit Markov Decision Processes while also trying out Q-Learning, the reinforcement learning approach that associates utilities with attempting actions in states.\n",
        "The problem that we’re attempting to solve is the following:\n",
        "\n",
        "1.  There is a grid of spaces in a rectangle.  Each space can contain a pit (negative reward), gold (positive reward), or nothing.\n",
        "2.  The rectangle is effectively surrounded by walls, so anything that would move you outside the rectangle, instead moves you to the edge of the rectangle.\n",
        "3.  The floor is icy.  Any attempt to move in a cardinal direction results in moving a somewhat random number of spaces in that direction.  The exact probabilities of moving each number of spaces are given in the problem description.  (If you slide too far, see rule #2.)\n",
        "4.  Landing on a pit or gold effectively “ends the run,” for both a Q learner and an agent later trying out the policy.  It’s game over.  (To simulate this during Q learning, set all Q values for the space to equal its reward, then start over from a random space.)  Note that it's still possible to slide past a pit or gold - this doesn't end the run.\n",
        "\n",
        "A sample input looks like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_PSiQiO-_2y"
      },
      "outputs": [],
      "source": [
        "sampleMDP = \"\"\"0.7 0.2 0.1\n",
        "- - P - -\n",
        "- - G P -\n",
        "- - P - -\n",
        "- - - - -\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CmLASMB_Gsd"
      },
      "source": [
        "\n",
        "The first line says that the probabilities of moving one, two, or three spaces in the direction of movement are 0.7, 0.2, and 0.1.   The rest is a map of the environment, where a dash is an empty space, P is a pit, and G is gold.\n",
        "\n",
        "Your job is to finish the code below for mdp_solve() and q_solve().  These take a problem description like the one pictured above, and return a policy giving the recommended action to take in each empty square (U=up, R=right, D=down, L=left).\n",
        "\n",
        "**1, 19 points)**  mdp_solve() should use value iteration and the Bellman equation.  ITERATIONS will refer to the number of complete passes you perform over all states.  You can initialize the utilities to the rewards of each state.  Don’t update the rewards spaces from their initial rewards; since they end the trial, they have no future utility.  Don't update utilities in-place as you iterate through them, but create a fresh array of utilities with each pass, in order to avoid biasing moves in the directions that have already been updated.\n",
        "\n",
        "**2, 26 points)**  q_solve() will run ITERATIONS trials in which a learner starts in a random empty square and moves until it hits a pit or gold, in which case, the trial is over.  (If it was randomly dropped into gold or a pit, the trial is immediately over.)  The learner moves by deciding randomly whether to choose a random direction (with probability EXPLORE_PROB) or move according to the best Q-value of its current square (otherwise).  Simulate the results of the move on slippery ice to determine where the learner ended up - then apply the Q-learning equation given in lecture and the textbook.  (There are multiple Q-learning variants out there, so try to use the equations and practices described in lecture instead of using other sources, to avoid confusion.)\n",
        "\n",
        "The fact that a trial ends immediately on finding gold or a pit means that we want to handle those spaces in a special way.  Normally Q values are updated on moving to the next state, but we won’t see any next state in these cases.  So, to handle this, when the agent discovers one of these rewards, set all the Q values for that space to the associated reward before quitting the trial.  So, for example, if gold is worth 100 and it’s discovered in square x, Q(x,UP) = 100, Q(x,RIGHT) = 100, Q(x, DOWN) = 100, and Q(x, LEFT) = 100.  There’s no need to apply the rest of the Q update equation when the trial is ending, because that’s all about future rewards, and there’s no future when the trial is ending.  But now the spaces that can reach that space will evaluate themselves appropriately.  (Before being \"discovered,\" the square should have no utility.)\n",
        "\n",
        "You should use the GOLD_REWARD, PIT_REWARD, LEARNING_RATE, and DISCOUNT_FACTOR constants at the top of the code box below.\n",
        "\n",
        "Q-learning involves a lot of randomness and some arbitrary decisions when breaking ties, so two implementations can both be correct but recommend slightly different policies in the end, even if they have the same starting random seed.  While we provide some helpful premade maps below, your main guide for debugging will be common sense in deciding whether the policy created by your agent makes sense -- ie, agents following the policy will get gold without taking unnecessary risks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZW7aHFXpUQ9l"
      },
      "outputs": [],
      "source": [
        "\"\"\" \"MDPs on Ice - Assignment 5\"\"\"\n",
        "\n",
        "import random\n",
        "import copy\n",
        "import numpy\n",
        "\n",
        "GOLD_REWARD = 250.0\n",
        "PIT_REWARD = -150.0\n",
        "DISCOUNT_FACTOR = 0.8\n",
        "EXPLORE_PROB = 0.2 # for Q-learning\n",
        "LEARNING_RATE = 0.01\n",
        "ITERATIONS = 20000\n",
        "MAX_MOVES = 1000\n",
        "ACTIONS = 4\n",
        "UP = 0\n",
        "RIGHT = 1\n",
        "DOWN = 2\n",
        "LEFT = 3\n",
        "MOVES = ['U', 'R', 'D', 'L']\n",
        "\n",
        "# Fixed random number generator seed for result reproducibility --\n",
        "# don't use a random number generator besides this to match sol\n",
        "random.seed(340)\n",
        "\n",
        "class Problem:\n",
        "    \"\"\"Represents the physical space, transition probabilities, reward locations, and approach\n",
        "\n",
        "    ...in short, the info in the problem string\n",
        "\n",
        "    Attributes:\n",
        "        move_probs (List[float]):  probabilities of going 1,2,3 spaces\n",
        "        map (List[List(string)]]:  \"-\" (safe, empty space), \"G\" (gold), \"P\" (pit)\n",
        "\n",
        "    String format consumed looks like\n",
        "    0.7 0.2 0.1   [probability of going 1, 2, 3 spaces]\n",
        "    - - - - - - P - - - -   [space-delimited map rows]\n",
        "    - - G - - - - - P - -   [G is gold, P is pit]\n",
        "\n",
        "    You can assume the maps are rectangular, although this isn't enforced\n",
        "    by this constructor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, probstring):\n",
        "        \"\"\" Consume string formatted as above\"\"\"\n",
        "        self.map = []\n",
        "        for i, line in enumerate(probstring.splitlines()):\n",
        "            if i == 0:\n",
        "                self.move_probs = [float(s) for s in line.split()]\n",
        "            else:\n",
        "                self.map.append(line.split())\n",
        "\n",
        "    def solve(self, iterations, use_q):\n",
        "        \"\"\" Wrapper for MDP and Q solvers.\n",
        "\n",
        "        Args:\n",
        "            iterations (int):  Number of iterations (but these work differently for the two solvers)\n",
        "            use_q (bool):  False means use MDP value iteration, true means use Q-learning\n",
        "        Returns:\n",
        "            A Policy, in either case (what to do in each square; see class below)\n",
        "        \"\"\"\n",
        "\n",
        "        if use_q:\n",
        "            return q_solve(self, iterations)\n",
        "        return mdp_solve(self, iterations)\n",
        "\n",
        "class Policy:\n",
        "    \"\"\" Abstraction on the best action to perform in each state.\n",
        "\n",
        "    This is a string list-of-lists map similar to the problem input, but a character gives the best\n",
        "    action to take in each non-reward square (see MOVES constant at top of file).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, problem):\n",
        "        \"\"\"Args:\n",
        "\n",
        "        problem (Problem):  The MDP problem this is a policy for\n",
        "        \"\"\"\n",
        "        self.best_actions = copy.deepcopy(problem.map)\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"Join the characters in the policy into one big space-separated, multline string\"\"\"\n",
        "        return '\\n{}\\n'.format('\\n'.join([' '.join(row) for row in self.best_actions]))\n",
        "\n",
        "def roll_steps(move_probs, row, col, move, rows, cols):\n",
        "    \"\"\"Calculates the new coordinates that result from a move.\n",
        "\n",
        "    Includes the \"roll of the dice\" for transition probabilities and checking arena boundaries.\n",
        "\n",
        "    Helper for try_policy and q_solve - probably useful in your Q-learning implementation.\n",
        "\n",
        "    Args:\n",
        "        move_probs (List[float]):  Transition probabilities for the ice (from problem)\n",
        "        row, col (int, int):  location of agent before moving\n",
        "        move (string):  The direction of move as a MOVES character (not an int constant!)\n",
        "        rows, cols (int, int):  number of rows and columns in the map\n",
        "\n",
        "    Returns:\n",
        "        new_row, new_col (int, int):  The new row and column after moving\n",
        "    \"\"\"\n",
        "    displacement = 1\n",
        "    total_prob = 0\n",
        "    move_sample = random.random()\n",
        "    for p, prob in enumerate(move_probs):\n",
        "        total_prob += prob\n",
        "        if move_sample <= total_prob:\n",
        "            displacement = p+1\n",
        "            break\n",
        "    # Handle \"slipping\" into edge of map\n",
        "    new_row = row\n",
        "    new_col = col\n",
        "    if not isinstance(move, str):\n",
        "        print(\"Warning: roll_steps wants str for move, got a different type\")\n",
        "    if move == \"U\":\n",
        "        new_row -= displacement\n",
        "        if new_row < 0:\n",
        "            new_row = 0\n",
        "    elif move == \"R\":\n",
        "        new_col += displacement\n",
        "        if new_col >= cols:\n",
        "            new_col = cols-1\n",
        "    elif move == \"D\":\n",
        "        new_row += displacement\n",
        "        if new_row >= rows:\n",
        "            new_row = rows-1\n",
        "    elif move == \"L\":\n",
        "        new_col -= displacement\n",
        "        if new_col < 0:\n",
        "            new_col = 0\n",
        "    return new_row, new_col\n",
        "\n",
        "def get_value(problem):\n",
        "    row = len(problem.map)\n",
        "    col = len(problem.map[0])\n",
        "    value = numpy.zeros((row, col))\n",
        "\n",
        "    for i in range(row):\n",
        "        for j in range(col):\n",
        "            if (problem.map[i][j] == \"P\"):\n",
        "                value[i][j] = PIT_REWARD\n",
        "            elif (problem.map[i][j] == \"G\"):\n",
        "                value[i][j] = GOLD_REWARD\n",
        "            else:\n",
        "                value[i][j] = -10\n",
        "    return value\n",
        "def prob(problem, x, y, j, k, action):\n",
        "    if(len(problem.move_probs) == 1):\n",
        "        if (action == \"U\"):\n",
        "            if((x-j) == 1)& ((k-y) == 0):\n",
        "                return 1\n",
        "            else:\n",
        "                return 0\n",
        "        if (action == \"D\"):\n",
        "            if((j-x)== 1)& ((k-y) == 0):\n",
        "                return 1\n",
        "            else:\n",
        "                return 0\n",
        "        if (action == \"R\"):\n",
        "            if((k-y)== 1)& ((x-j) == 0):\n",
        "                return 1\n",
        "            else:\n",
        "                return 0\n",
        "        if (action == \"L\"):\n",
        "            if((y-k)== 1)& ((x-j) == 0):\n",
        "                return 1\n",
        "            else:\n",
        "                return 0\n",
        "    if (len(problem.move_probs) == 2):\n",
        "        if (action == \"U\"):\n",
        "            if((x-j) == 1)& ((k-y) == 0):\n",
        "                return problem.move_probs[0]\n",
        "            elif((x-j) == 2)& ((k-y) == 0):\n",
        "                return problem.move_probs[1]\n",
        "            else:\n",
        "                return 0\n",
        "        if (action == \"D\"):\n",
        "            if((j-x)== 1)& ((k-y) == 0):\n",
        "                return problem.move_probs[0]\n",
        "            elif((j-x)== 2)& ((k-y) == 0):\n",
        "                return problem.move_probs[1]\n",
        "            else:\n",
        "                return 0\n",
        "        if (action == \"R\"):\n",
        "            if((k-y)== 1)& ((x-j) == 0):\n",
        "                return problem.move_probs[0]\n",
        "            elif((k-y)== 2)& ((x-j) == 0):\n",
        "                return problem.move_probs[1]\n",
        "            else:\n",
        "                return 0\n",
        "        if (action == \"L\"):\n",
        "            if((y-k)== 1)& ((x-j) == 0):\n",
        "                return problem.move_probs[0]\n",
        "            elif((y-k)== 2)& ((x-j) == 0):\n",
        "                return problem.move_probs[1]\n",
        "            else:\n",
        "                return 0\n",
        "    if(len(problem.move_probs) == 3):\n",
        "        if (action == \"U\"):\n",
        "            if((x-j) == 1)& ((k-y) == 0):\n",
        "                return problem.move_probs[0]\n",
        "            elif((x-j) == 2)& ((k-y) == 0):\n",
        "                return problem.move_probs[1]\n",
        "            elif((x-j) == 3)& ((k-y) == 0):\n",
        "                return problem.move_probs[2]\n",
        "            else:\n",
        "                return 0\n",
        "        if (action == \"D\"):\n",
        "            if((j-x)== 1)& ((k-y) == 0):\n",
        "                return problem.move_probs[0]\n",
        "            elif((j-x)== 2)& ((k-y) == 0):\n",
        "                return problem.move_probs[1]\n",
        "            elif((j-x)== 3)& ((k-y) == 0):\n",
        "                return problem.move_probs[2]\n",
        "            else:\n",
        "                return 0\n",
        "        if (action == \"R\"):\n",
        "            if((k-y)== 1)& ((x-j) == 0):\n",
        "                return problem.move_probs[0]\n",
        "            elif((k-y)== 2)& ((x-j) == 0):\n",
        "                return problem.move_probs[1]\n",
        "            elif((k-y)== 2)& ((x-j) == 0):\n",
        "                return problem.move_probs[2]\n",
        "            else:\n",
        "                return 0\n",
        "        if (action == \"L\"):\n",
        "            if((y-k)== 1)& ((x-j) == 0):\n",
        "                return problem.move_probs[0]\n",
        "            elif((y-k)== 2)& ((x-j) == 0):\n",
        "                return problem.move_probs[1]\n",
        "            elif((y-k)== 2)& ((x-j) == 0):\n",
        "                return problem.move_probs[1]\n",
        "            else:\n",
        "                return 0\n",
        "\n",
        "def try_policy(policy, problem, iterations):\n",
        "    \"\"\"Returns average utility per move of the policy.\n",
        "\n",
        "    Average utility is as measured by \"iterations\" random drops of an agent onto empty\n",
        "    spaces, running until gold, pit, or time limit MAX_MOVES is reached.\n",
        "\n",
        "    Doesn't necessarily play a role in your code, but you can try policies this\n",
        "    way\n",
        "\n",
        "    Args:\n",
        "        policy (Policy):  the policy the agent is following\n",
        "        problem (Problem):  the environment description\n",
        "        iterations (int):  the number of random trials to run\n",
        "    \"\"\"\n",
        "    total_utility = 0\n",
        "    total_moves = 0\n",
        "    for _ in range(iterations):\n",
        "        # Resample until we have an empty starting square\n",
        "        while True:\n",
        "            row = random.randrange(0, len(problem.map))\n",
        "            col = random.randrange(0, len(problem.map[0]))\n",
        "            if problem.map[row][col] == \"-\":\n",
        "                break\n",
        "        for moves in range(MAX_MOVES):\n",
        "            total_moves += 1\n",
        "            policy_rec = policy.best_actions[row][col]\n",
        "            # Take the move - roll to see how far we go, bump into map edges as necessary\n",
        "            row, col = roll_steps(problem.move_probs, row, col, policy_rec, \\\n",
        "                                  len(problem.map), len(problem.map[0]))\n",
        "            if problem.map[row][col] == \"G\":\n",
        "                total_utility += GOLD_REWARD\n",
        "                break\n",
        "            if problem.map[row][col] == \"P\":\n",
        "                total_utility += PIT_REWARD\n",
        "                break\n",
        "    return total_utility / total_moves\n",
        "\n",
        "def mdp_solve(problem, iterations):\n",
        "    \"\"\" Perform value iteration for the given number of iterations on the MDP problem.\n",
        "\n",
        "    Here, the squares with rewards can be initialized to the reward values, since value iteration\n",
        "    assumes complete knowledge of the environment and its rewards.\n",
        "\n",
        "    Args:\n",
        "        problem (Problem):  description of the environment\n",
        "        iterations (int):  number of complete passes over the utilities\n",
        "    Returns:\n",
        "        a Policy (though you may design this to return utilities as a second return value)\n",
        "    \"\"\"\n",
        "    r = get_value(problem)\n",
        "    row = len(problem.map)\n",
        "    col = len(problem.map[0])\n",
        "    value = numpy.zeros((row, col))\n",
        "    ip = Policy(problem)\n",
        "    for i in range(iterations):\n",
        "        nvalue = numpy.zeros((row, col))\n",
        "        for j in range(row):\n",
        "            for k in range(col):\n",
        "                if(problem.map[j][k]!=\"P\"):\n",
        "                    mv = 0\n",
        "                    for o in MOVES:\n",
        "                        val = r[j][k]\n",
        "                        for t in range(row):\n",
        "                            for y in range(col):\n",
        "                                val += (prob(problem, j, k, t, y, o) * DISCOUNT_FACTOR*value[t][y])\n",
        "                        mv = max(mv, val)\n",
        "                        if (value[j][k] < val):\n",
        "                            if(problem.map[j][k]==\"-\"):\n",
        "                                ip.best_actions[j][k] = o\n",
        "                    nvalue[j][k] = mv\n",
        "        value = nvalue\n",
        "    return ip\n",
        "def explore(per):\n",
        "    percent = per*100\n",
        "    pro = random.randrange(0,100)\n",
        "    if pro < percent:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "def opposite(move):\n",
        "    if(isinstance(move, str)):\n",
        "        if(move == \"U\"):\n",
        "            return \"D\"\n",
        "        if(move == \"D\"):\n",
        "            return \"U\"\n",
        "        if(move == \"R\"):\n",
        "            return \"L\"\n",
        "        if(move == \"L\"):\n",
        "            return \"R\"\n",
        "    else:\n",
        "        if(move == 0):\n",
        "            return 2\n",
        "        if(move == 2):\n",
        "            return 0\n",
        "        if(move == 1):\n",
        "            return 3\n",
        "        if(move == 3):\n",
        "            return 1\n",
        "\n",
        "def q_solve(problem, iterations):\n",
        "    \"\"\"q_solve:  Use Q-learning to find a good policy on an MDP problem.\n",
        "\n",
        "    Each iteration corresponds to a random drop of the agent onto the map, followed by moving\n",
        "    the agent until a reward is reached or MAX_MOVES moves have been made.  When an agent\n",
        "    is sitting on a reward, update the utility of each move from the space to the reward value\n",
        "    and end the iteration.  (For simplicity, the agent also does this if just dropped there.)\n",
        "    The agent does not \"know\" reward locations in its Q-values before encountering the space\n",
        "    and \"discovering\" the reward.\n",
        "\n",
        "    Note that texts differ on when to pay attention to this reward - this code follows the\n",
        "    convention of scoring rewards of the space you are moving *from*, plus discounted best q-value\n",
        "    of where you landed.\n",
        "\n",
        "    Assume epsilon-greedy exploration.  Leave reward letters as-is in the policy,\n",
        "    to make it more readable.\n",
        "\n",
        "    Args:\n",
        "        problem (Problem):  The environment\n",
        "        iterations (int):  The number of runs from random start to reward encounter\n",
        "    Returns:\n",
        "        A Policy for the map\n",
        "    \"\"\"\n",
        "    row = len(problem.map)\n",
        "    col = len(problem.map[0])\n",
        "    q_list = {}\n",
        "    for i in range(row):\n",
        "        for j in range(col):\n",
        "            q_list[(i,j)] = [problem.map[i][j], [0]*len(MOVES)]\n",
        "    for i in range(iterations):\n",
        "        tm = 0\n",
        "        f = random.randint(0, row)\n",
        "        b = random.randint(0, col)\n",
        "        start_node = [f,b]\n",
        "        run = True\n",
        "        while(run):\n",
        "            if (tm>=MAX_MOVES):\n",
        "                run = False\n",
        "                break\n",
        "            tm +=1\n",
        "            if(explore(0.2)):\n",
        "\n",
        "                x = random.randint(0, 3)\n",
        "                if()\n",
        "                t, y = roll_steps(problem.move_probs, f, b, MOVES[x], row, col)\n",
        "                if(problem.map[t][y]==\"-\"):\n",
        "                    q_list[(f,b)][1][x] = (1-LEARNING_RATE)*q_list[(f,b)][1][x] + LEARNING_RATE *(-10 + DISCOUNT_FACTOR * max(q_list[(f,b)][1][0], q_list[(f,b)][1][1], q_list[(f,b)][1][2], q_list[(f,b)][1][3]))\n",
        "\n",
        "                elif(problem.map[t][y]==\"P\"):\n",
        "                    q_list[(f,b)][1][x] = (1-LEARNING_RATE)*q_list[(f,b)][1][x] + LEARNING_RATE *(PIT_REWARD + DISCOUNT_FACTOR * max(q_list[(f,b)][1][0], q_list[(f,b)][1][1], q_list[(f,b)][1][2], q_list[(f,b)][1][3]))\n",
        "                    run = False\n",
        "                else:\n",
        "                    q_list[(f,b)][1][x] = (1-LEARNING_RATE)*q_list[(f,b)][1][x] + LEARNING_RATE *(GOLD_REWARD + DISCOUNT_FACTOR * max(q_list[(f,b)][1][0], q_list[(f,b)][1][1], q_list[(f,b)][1][2], q_list[(f,b)][1][3]))\n",
        "                    run = False\n",
        "    policy = Policy(problem)\n",
        "    for i in range(row):\n",
        "        for j in range(col):\n",
        "            if(problem.map[i][j] == \"-\"):\n",
        "                mv = max(q_list[(f,b)][1])\n",
        "                if(mv == q_list[(f,b)][1][0]):\n",
        "                    policy.best_actions[i][j] = \"U\"\n",
        "                if(mv == q_list[(f,b)][1][1]):\n",
        "                    policy.best_actions[i][j] = \"R\"\n",
        "                if(mv == q_list[(f,b)][1][2]):\n",
        "                    policy.best_actions[i][j] = \"D\"\n",
        "                if(mv == q_list[(f,b)][1][3]):\n",
        "                    policy.best_actions[i][j] = \"L\"\n",
        "    return policy\n",
        "\n",
        "def new_q(rewards, utilities, r, c, new_r, new_c, movenum):\n",
        "    \"\"\" Q-learning function.  Returns the new Q-value for space (r,c).\n",
        "    It's recommended you code and test this before doing the overall Q-learning.\n",
        "\n",
        "    Should use the LEARNING_RATE and DISCOUNT_FACTOR.\n",
        "\n",
        "    Args:\n",
        "        rewards (List[List[float]]):  Reward amounts built into the problem map (indexed [r][c])\n",
        "        utilities (List[List[List[float]]]):  The Q-values for each action from each space.\n",
        "                                              (Indexed as [row][col][move])\n",
        "        r, c (int, int):  Row and column of our location before move\n",
        "        new_r, new_c (int, int):  Row and column of our location after move\n",
        "        movenum (int):  Integer index into the Q-values, corresponding to constants UP etc\n",
        "    Returns:\n",
        "        float - the new Q-value for the space we moved from\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    return new_q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cuaEcLvAoYK"
      },
      "outputs": [],
      "source": [
        "deterministic_test = \"\"\"1.0\n",
        "- - P - -\n",
        "- - G P -\n",
        "- - P - -\n",
        "- - - - -\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ABkxRiTA4Wi"
      },
      "outputs": [],
      "source": [
        "# Notice that we counterintuitively are most likely to go 2 spaces here\n",
        "very_slippy_test = \"\"\"0.2 0.7 0.1\n",
        "- - P - -\n",
        "- - G P -\n",
        "- - P - -\n",
        "- - - - -\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lqg8ZZUCBYYl"
      },
      "outputs": [],
      "source": [
        "big_test = \"\"\"0.6 0.3 0.1\n",
        "- P - G - P - - G -\n",
        "P G - P - - - P - -\n",
        "P P - P P - P - P -\n",
        "P - - P P - - - - P\n",
        "- - - - - - - - P G\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHZ99I9uBmiH",
        "outputId": "c4085092-69ed-45fd-e061-0cf613a1d2f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "D D P R D\n",
            "R R G P D\n",
            "R U P D L\n",
            "R U L L L\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# MDP value iteration tests\n",
        "print(Problem(deterministic_test).solve(ITERATIONS, False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txLGS4pUwhh7",
        "outputId": "243bc37b-d35c-41cc-b765-a1d31192c634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "D D P L D\n",
            "R R G P L\n",
            "U U P D U\n",
            "U U L L L\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(Problem(sampleMDP).solve(ITERATIONS, False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnprAX2uwiDI",
        "outputId": "57af45e0-c122-439a-b1b3-cc99fcf7b99b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "D D P L D\n",
            "R D G P L\n",
            "U U P L U\n",
            "U R U L U\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(Problem(very_slippy_test).solve(ITERATIONS, False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INhKxA6twic8",
        "outputId": "f8b58c9f-c9e1-4b92-87fe-ff66bf0c31a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "R P R G L P R R G L\n",
            "P G L P U R U P U U\n",
            "P P U P P U P U P U\n",
            "P U U P P R U U U P\n",
            "R U U L L R R R P G\n",
            "\n"
          ]
        }
      ],
      "source": [
        "policy_big = Problem(big_test).solve(ITERATIONS, False)\n",
        "print(policy_big)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfUJKMPtCRCs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "3b81765e-290f-4345-9536-9b8038007ab8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-00218a0125fa>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# comment out to get different random runs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m340\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mProblem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeterministic_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mITERATIONS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-47-14c1001ef703>\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, iterations, use_q)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_q\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mq_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmdp_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-14c1001ef703>\u001b[0m in \u001b[0;36mq_solve\u001b[0;34m(problem, iterations)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                 \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroll_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMOVES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mDISCOUNT_FACTOR\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "# Q-learning tests\n",
        "# Set seed every time for consistent executions;\n",
        "# comment out to get different random runs\n",
        "random.seed(340)\n",
        "print(Problem(deterministic_test).solve(ITERATIONS, True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08cHCoI6wqak",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "0d9899a1-b731-46c8-dff4-78581a4630bd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-23f753020d84>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m340\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mProblem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampleMDP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mITERATIONS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-47-14c1001ef703>\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, iterations, use_q)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_q\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mq_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmdp_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-14c1001ef703>\u001b[0m in \u001b[0;36mq_solve\u001b[0;34m(problem, iterations)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                 \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroll_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMOVES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mDISCOUNT_FACTOR\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "random.seed(340)\n",
        "print(Problem(sampleMDP).solve(ITERATIONS, True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMM3kelxwqsx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "5d06f141-bef5-4200-ba89-13f0ea0dd611"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-e43bb18f09d1>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m340\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mProblem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvery_slippy_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mITERATIONS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-47-14c1001ef703>\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, iterations, use_q)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_q\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mq_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmdp_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-14c1001ef703>\u001b[0m in \u001b[0;36mq_solve\u001b[0;34m(problem, iterations)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                 \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroll_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMOVES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mDISCOUNT_FACTOR\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "random.seed(340)\n",
        "print(Problem(very_slippy_test).solve(ITERATIONS, True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWu_w30AwrP9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "ee96ec44-322e-4189-a735-66344f2e5240"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-00fcc9656bf3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m340\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mProblem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbig_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mITERATIONS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-47-14c1001ef703>\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, iterations, use_q)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_q\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mq_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmdp_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-14c1001ef703>\u001b[0m in \u001b[0;36mq_solve\u001b[0;34m(problem, iterations)\u001b[0m\n\u001b[1;32m    376\u001b[0m                 \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroll_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMOVES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m                     \u001b[0mq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mDISCOUNT_FACTOR\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"P\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: (3, 10)"
          ]
        }
      ],
      "source": [
        "random.seed(340)\n",
        "print(Problem(big_test).solve(ITERATIONS, True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbP5VrvF9dIt"
      },
      "source": [
        "Once you're done, here are a few thought questions (15 points total):\n",
        "\n",
        "**3, 3 points) Suppose we are on the deterministic map where there is no sliding on ice, and performing value iteration until it converges.  Supposing 0 < DISCOUNT_FACTOR < 1, how does the policy change if the discount factor changes to another value in that range (or does the policy change at all)?  Why does that happen?  What happens to the policy if DISCOUNT_FACTOR = 1?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzD5X8wY-5gS"
      },
      "source": [
        "Given that the discount is used to create a dimishing return in the the value of a given choice it made since that as decreased the discount value there where more and more values thet didn't recieve a path due to the value decreasing so low for values far from gold the paths where worth it to the program. Oddly enough I found that in policy_big when discount was 1.0 the program had no issue attemping to go through a pit to get to a reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CkLwZnX-87e"
      },
      "source": [
        "**4, 3 points) The value iteration MDP solver updates all squares an equal number of times.  The Q-learner does not.  Which squares might we expect the Q-learner to update the most?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEhHogqO_7fC"
      },
      "source": [
        "I expect the squares that are further from the reward squares to be the ones that get changed the most often as their values are more likely to change."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeGcHwKR_-m7"
      },
      "source": [
        "**5, 9 points) Suppose we change the state information so that, instead of knowing its coordinates on the map, the agent instead knows just the locations of all rewards in a 5x5 square with the agent at the square center.  Thus, at the start of every run, it may not know exactly where it is, but it knows what is in the vicinity.  It also does not know the transition model.**\n",
        "\n",
        "**a, 3 points) We can't use value iteration here.  Why?**\n",
        "\n",
        "**b, 3 points) How many state-action combinations are possible, assuming the contents of the agent's own square don't matter?  Is a lookup table of Q-values feasible if we allocate memory for each possible state-action combination?  (Let's define \"feasible\" as \"able to be stored in a gig or less of memory,\" assuming 64-bit values.)**\n",
        "\n",
        "**c, 3 points) What is the most likely method that we would use to generalize in Q-learning from seen state-action combinations to situations that haven't been encountered before?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Inb8brIUIk8U"
      },
      "source": [
        "**a) we can't use value iteration because value iteration stems on the fact that we know of all of the information about the given state to all the agent a path no matter where it starts\n",
        "\n",
        "**b) given that know where the location of the agent is other than its adjancency to the reward the state there should be 25 given states in the 5X5 and with 4 actions that would mean 100 state action combinations. Thus i would believe that its completely feasible that it wouls be able to compute given i got big_test to run.\n",
        "\n",
        "**c) We could see all problems like this as all given information as the state of the problem, this is everything that comes before the action then a list of the possible action would work in the same way its alway's had"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78PjFoZBOLfp"
      },
      "source": [
        "**Remember to submit your code on Blackboard as both an .ipynb (File->Download->.ipynb) and a PDF (Print->Save as PDF).**"
      ]
    }
  ],
  "metadata": {
    "author": [
      {
        "@type": "Person",
        "name": "Kevin Gold"
      }
    ],
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}